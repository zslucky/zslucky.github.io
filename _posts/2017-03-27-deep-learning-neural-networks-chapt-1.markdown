---
layout: math-post
title: Deep learning - Chapt 1 - Using neural nets to recognize handwritten digits
chineseTitle: 深度学习 - 第一章 - 使用神经网来识别手写数字
icon: microchip
date:   2017-03-26 10:50:57 +0800
categories:
  - deep-learning
tags:
  - python
---

## Overview

**[转][译]** [《Neural Networks and Deep Learning - Chapt 1》](http://neuralnetworksanddeeplearning.com/chap1.html) --- [Michael Nielsen](http://michaelnielsen.org/)

人类的视觉系统是这个世界上最为奇妙的事物之一，面对一些手写的数字，人们可以很轻易的识别它，然而这种轻松是具有欺诈性的。在我们每一半大脑中，人们都有一个主要的视觉皮层也就是我们已知的V1(初级视皮层，亦称纹状皮层)，包含了1.4亿个神经元以及100亿个相互作用的连接。当然，人类的视力还不仅仅只是是拥有V1，而是一整个系列的视觉皮层 -- V2、V3、V4和V5会连续渐进的进行复杂的图像处理。我们给我们的头脑携带了一个超级电脑，他经历了数亿年的进化，使自己适应并理解这个可视的世界，所以辨识手写数字并不简单。我们人类是神奇的，擅长于理解我们眼睛所看的东西，但是完成这些工作却是不经意间的，令人惊讶。所以我们通常无法领会我们的视觉系统如何解决这样困难的问题。

如果你尝试去写出一个类似以上描述的电脑程序来辨识数字，视觉辨识部分很明显是很困难的。在我们看来似乎很简单的东西突然变得极为困难，简单的凭直觉如何来辨识外形 - “一个数字9，上面有一个圈，右下方又一个竖线”，结果是并不能那么简单的在算法上表示出来。当你尝试去明确这些规则，你很快会迷失在一堆异议、警告和特殊情况的困境中，这是不希望的。

神经网络利用了不同的方法来着手处理这个问题，就是利用大量的手写数字作为已知的训练样本

![mnist_100_digits]({{ site.url }}/static/deep-learning-chapt-1/mnist_100_digits.png)

然后开发一个系统来学习这些训练样本，换句话说，神经网络利用样本来自动的推出规则来辨识手写数字，更进一步的说就是，利用不断增多的悬链样本，神经网络能够学到更多关于手写内容来提升准确率，因此我已经展示了以上100个训练数字，也许我们可以利用成千上万甚至上亿的训练样本来创建一个更好的手写识别器。

在这一章节中，我们会编写一个电脑程序来实现这个神经网络来学习识别手写数字，这段程序只有74行，不利用任何神经网络库，但这个小程序能够在没有人介入并拥有96%准确率的情况下来识别手写数字，进一步说在之后的章节中我们会开发新的主意来提升性能至99%以上。实际上，最好的商业神经网络现在已经被很好的使用在银行中来处理支票，利用邮件办公室来识别地址。

我们致力于手写内容识别是因为它可以作为一个通用的神经网络模型，他可以用来开发更高级的技术，比如深度学习。在之后的内容中，我们会讨论这些点子是如何被应用在计算机视觉、语音、自然语言处理甚至是其他领域。

当然，如果只是为了写一个程序来识别手写数字，那么这章节就太短了！所以沿途我们会开发许多关关于神经网络键的点子，包括2个重要的人造神经元类型(感知器和sigmoid神经元)，还有标准的神经网络学习算法，如已知的SGD(随机梯度下降算法)，全文我都会专注于解释为什么事物会被这样完成，还有建立你的神经网络直觉。比起如果我只是陈述基础的原理，那更需要一个漫长的讨论，但是对你达到深入理解是很有价值的，在章节结束我们能够理解什么是深度学习，还有为什么他有影响。

---

## Contents

- [**Perceptrons** - 感知器](#perceptrons)
- [**Sigmoid Neurons** - Sigmoid神经元](#sigmoid-neurons)
  - [**Exercises 1** - 练习1](#exercises-1)
- [**The architecture of neural networks** - 神经网络结构](#the-architecture-of-neural-networks)
- [**A Simple network to classify handwritten digits** - 一个分类手写数字的简单网络](#a-simple-network-to-classify-handwritten-digits)
  - [**Exercises 2** - 练习2](#exercises-2)

## Perceptrons

---

什么是神经网络？作为开始，我会解释一个叫做感知器的人工神经，感知器是在1950和1960年由[Frank Rosenblatt](https://en.wikipedia.org/wiki/Frank_Rosenblatt)开发，受到更早[Warren McCulloch](https://en.wikipedia.org/wiki/Warren_Sturgis_McCulloch)和][Walter Pitts](https://en.wikipedia.org/wiki/Walter_Pitts)的启发。如今它已经非常普遍的在其他模型中使用人工神经元 - 在本书中，并且在大多数神经网络模型中，主要使用的是一个叫Sigmoid神经元的东西。我们会对sigmoid有个简短的接触，但是可以让我们了解为什么sigmoid神经元被定义为这种工作方式，让我们一起来花点时间来看下第一个感知器。

感知器怎么工作？一个感知器有多个二进制输入$$ x_1 $$, $$ x_2 $$, ..., 并且处理成单独一个二进制输出：

![First-perceptron]({{ site.url }}/static/deep-learning-chapt-1/first-perceptron.png)

这个例子中显示的感知器有3个输入$$ x_1 $$, $$ x_2 $$和$$ x_3 $$，通常来说它可以有更多的输入，Rosenblatt提出一个简单的规则来计算输出，他介绍了权重$$ \text{weights } $$, $$ w_2 $$和$$ w_2 $$, ...，真实的数字表示各个输入对于输出的重要性，神经元的输出是0或1由权重总和$$ \sum_jw_jx_j $$是否小于或大于某个阈值来决定，就比如权重，这个阈值是神经元的一个真实的数据，明确一点的代数表达式如下：

$$
output =
\begin{cases}
0 & \text{if } \sum_jw_jx_j  \leq \text{ threshold} \\
1 & \text{if } \sum_jw_jx_j  > \text{ threshold}
\end{cases}
$$

这感知器如何工作的全部。

这是个基础的数学模型，你可以把感知器认为是一个可以权衡证据来做决策的设备，这还不是一个非常现实的示例，但他可以让你简单的理解，我们很亏会接触一个更现实的例子。假设下周末即将来临，你已经听说了在你的城市会举办一个国际象棋活动，你也喜欢国际象棋，你在尝试做决定是否要参加这个活动，你做决定可能会权衡以下3个因素：

1. 天气是否晴朗？
2. 你的男／女朋友是否陪你一起？
3. 活动地点是否在公共交通设施附近？（你没有自己的车）

我们可以把这3个因素展示成二进制的变量$$ x_1 $$, $$ x_2 $$和$$ x_3 $$, 在这个实例中，如果天气好我们有$$x_1 = 1 $$, 如果天气不好我们有$$x_1 = 0$$, 同样的$$ x_2 = 1$$代表如果你的男／女朋友也想去，而$$ x_2 = 0$$表示不去，同样的也可以用在公共交通设施上。

现在，假设你是绝对的喜欢国际象棋，甚至说你去参加活动无关男／女朋友他是否对活动感兴趣或者去达场地有多困难，但是你真的非常讨厌坏天气，而且如果天气不好你没有办法去活动场地，你可以使用感知器来完成这个决策模型，一种发式是选择天气的权重$$ w_1 = 6 $$, 其他的设为$$ w_2 = 2, w_3 = 2 $$，$$ w_1 $$这个更大点的说字说明了他影响你更多，比男女朋友陪你参加货距离公共交通设施更近更重要，最后，你选择了一个阈值为5的感知器，根据这些选择，感知器得到了一个决策模型，只要天气好就输出1，只要天气差就输出0，这个无关另外两个条件。

利用多样的权重和阈值，我们可以得到不同的决策模型，比如我们利用阈值3来代替，那么我们的感知器会决定当天气是好的或者另外两个条件同时满足时你去参加活动，换句话说，这就是一个不同的决策模型了，降低阈值代表了你有更强烈的欲望去参加活动。

很明显，感知器不适宜颚完成的人类决策模型，但这个例子阐明了感知器是如何权衡各种类型的证据来做决策的，并且一个复杂的感知器网络可以做一个相当精确的决策，这个决策还是比较可信的。

![Perceptron-Network]({{ site.url }}/static/deep-learning-chapt-1/multi-perceptron-network.png)

在这个网络中，第一列感知器（我们称之为第一层感知器），会根据输入的证据做3个非常简单的决策，那么第二层感知器呢？每一个感知器会权衡第一层做的决定结果做一个决策，这样的话第二层的感知器可以做一个比第一层感知器更复杂更抽象级别的决策，而且还可以在第三层感知器上做更复杂的决定，这么一来，一个多层感知器网络可以作用于更复杂的决策。

顺带一提，当我定义多个感知器的时候我说过一个感知器直邮一个输出，在上面的网络中感知器看上去有多个输出，实际上她们仍然是单个输出，这多个输出箭头仅仅只是表示某个感知器的输出作为其他多个感知器的输入，

让我们简化感知器的描述，条件$$ \sum_jw_jx_j > \text{threshold} $$是笨重的，我们可以创造2个计数变化来简化它，第一个变化是将$$ \sum_jw_jx_j $$写成一个点积$$ w\cdot x = \sum_jw_jx_j $$, 这里$$ w $$和$$ x $$是权重和输入的向量，第二个改变是将$$ \text{threshold} $$改动到不等式的另一边，并且把它改成我们已知的感知器的偏置值$$ bias $$，$$ b \equiv -\text{threshold} $$，利用偏置值来代替阈值，感知器规则可以被重写成：

$$
output =
\begin{cases}
0 & \text{if } w\cdot x + b  \leq 0 \\
1 & \text{if } w\cdot x + b  > 0
\end{cases}
$$

你可以将偏置值想成是一个感知器有多简单输出1的测量标准，或者说测量一个感知器有多容易被触发，当感知器拥有一个相当大的偏置值时，他会非常容易的输出1，但是当它为负时，感知器就很难输出1了，很明显偏置值在我们描述感知器的时候只会有一个小的改变，但是我们会在之后看到它进一步简单化计算。正是因为如此，这本书中我们不会再用阈值，我们会一直使用偏置值$$ bias $$。

我已经描述过感知器是一个权衡证据来做决策的方法，另一方面，感知器可以被用来计算我们经常隐藏于计算下的基础逻辑功能，功能比如 AND, OR和NAND，举个例子，假设我们友谊颚感知器有两个输出，权重都是-2，并且全部的偏置值时3，如下图：

![elementary-logical-function]({{ site.url }}/static/deep-learning-chapt-1/elementary-logical-function.png)

然后我们输入00，便输出了1，因为$$ (-2) * 0 + (-2) * 0 + 3 = 3 $$是正数，我已经介绍过符号*是显式的乘法，同样的计算输入为01和10时产出1，但是输入为11的时候产出是0，因为$$ (-2) * 1 + (-2) * 1 + 3 = -1 $$是一个负值，因此我们的感知器实现了一个NAND（与非门）！

NAND与非门例子表明我们可以用感知器来计算简单的逻辑功能，实际上我们可以使用的感知器网络来计算任何逻辑功能，原因是因为与非门是通用的计算，那就是说，我们可以基于与非门创立任何的计算，比如，我们可以利用与非门来创建一个添加2个比特的电路，$$ x_1 $$和$$ x_2 $$，这就需要进行按位求和(bitwise sum), $$ x_1 \bigoplus x_2 $$，同样的当$$ x_1 $$和$$ x_2 $$同为1将会被设置为1时进位位(carry bit)会被设置成1，就好比进位位时它们的积$$ x_1x_2 $$：

![bitwise-and-carry]({{ site.url }}/static/deep-learning-chapt-1/bitwise-and-carry.png)

为了得到一个相等的感知器网络，我们将所有的与非门替换成2个输入的感知器，每个的权重为-2偏置值为3，以下就是这个网络：

![replaced-nand-network]({{ site.url }}/static/deep-learning-chapt-1/replaced-nand-network.png)

感知器网络中一个值得注意的方面是最左边的感知器的输出被2次用来作为最下边感知器的输入，当我定义感知器模型时，我不曾说过是否这种“两次输出到同一地方”是被允许的，实际上，这没什么太大关系，如果我们不希望允许这类事，那么我们也许可以简单的将两条线合并到一条中去，并且设置权重为-4来代替-2（如果你没很明显的发现，你因该先停下来然后证明给你自己看这是相等的），根据这些改动，这图看上去如下，所有未标记的权重都是-2，所有偏置值都是3，仅有一个权重为-4:

![merged-output-network]({{ site.url }}/static/deep-learning-chapt-1/merged-output-network.png)

到目前为止，我已经在感知器左边绘制出了像$$ x_1 $$和$$ x_2 $$这样的变量，实际上给感知器在额外绘制一个层更符合传统 -- 输入层，来给输入编码：

![input-layer-network]({{ site.url }}/static/deep-learning-chapt-1/input-layer-network.png)

这个感知器输入的符号它有一个输出，但是没有输入：

![input-layer]({{ site.url }}/static/deep-learning-chapt-1/input-layer.png)

这是一个简化部分，他并不意味着感知器可以没有输入，看这个，我们只是假设有这么一个感知器没有输入，然后这个权重和总是为0，$$ \sum_jw_jx_j $$, 因此如果$$ b > 0 $$感知器会输出1，如果$$ b \leq 0 $$感知器会输出0，就是说感知器会简单的输出一个修正过的数值，而不是所期望的值(比如之前的$$ x_1 $$), 因此我们可以更好的把输入感知器不当成感知器，但是是一个更特殊的单元，他只是简单的定义了一个我们所期望的输出值$$ x_1, x_2, $$...

这个计数器例子展示了感知器网络如何使用多个NAND与非门来模拟电路，因为与非门是普遍的计算，所以感知器也是通用的计算。

这种感知器的通用性计算，既让我们安心又让我们失望，让我们安心是因为他告诉我们感知器网络可以像其他计算设备一样强大，但是同样也让我们失望是因为他让我们看起来感知器仅仅只是一个新的与非门，他几乎不是什么大的新东西！

但情况比这篇陈述的观点要好，他给了我们一个结果，我们可以设计一个自动调整人工神经元网络权重weights和偏置值biases的学习算法，这个调整发生在受到外部刺激时，而不是有程序人员的直接介入，这种学习算法让我们有了一种方式能够使用与传统逻辑门完全不同的人工神经元，替代显式的与非门或是其他门展示的电路，我们的神经网络能很轻松的来学习解决问题，有时候直接的设计传统电路时非常困难的一个问题。

## Sigmoid Neurons

---

学习算法听起来很可怕，但我们怎么能设计一个这样的神经网络算法呢？假设我们有一个感知器网络，我们要用来学习去解决一些问题，比如，这个网络可能会通过扫描输入一些手写数字图片的像素源数据，然后我们希望网络来学习权重和偏置值来实现对输出数字的正确分类，来看下这个学习时怎么工作的，假设我们对权重或是偏置值做一些小的改动，我们希望看到的是他只会对输出产生一个相应很小的变化，我们会在之后看到这个属性将会使学习成为可能，按计划，这就是我们想要的（很显然这个网络来做手写内容分类太过于简单了！）

![first-handwriting-recognition]({{ site.url }}/static/deep-learning-chapt-1/first-handwriting-recognition.png)

如果权重或是偏置值的小变动引起输出的小变动成立的话，那么我们就可以利用改动权重和偏置值来让我们的网络按照我们想要的方式运行，比如，假设网络分类8时犯了个错，他应该是9，那么我们就应该指出他并且对权重和偏置值做个小改动来让她更接近于可以分类识别出9，之后不断重复他，不断的改变权重和偏置值来产出更好的输出，网络便学习了。

这里的问题是当我们的网络包含了感知器时的表现并不是这么表现的，实际上，任何一个感知器的权重或者偏置值的一个小变动有时会引起感知器输出完全跳跃，就是说0到1，那么这种跳跃可能会引起网络行为的完全改变，所以你的9可能会被正确识别了，但是你的网络对其他图片的识别会变的难以控制，这就导致他很难看到他如何渐进的改变权重和偏置值来得到我们想要的行为，也许这里有一些聪明的方式来避开这个问题，但是他不能立刻明显的看出我们是如何让感知器网络去学习的。

我们利用一个叫$$ sigmoid $$神经元的新类型人工神经元来克服这个问题，Sigmoid神经元蕾丝与感知器，但是他的改动可以让权重或偏置值的变动只对输出产生很小的影响，这是一个允许引用sigmoid神经元网络来学习的重要原因。

好吧，接下来让我来介绍下sigmoid神经元，我们会像描述感知器那样描述sigmoid神经元：

![First-perceptron]({{ site.url }}/static/deep-learning-chapt-1/first-perceptron.png)

就像一个感知器，sigmoid神经元有多个输入，$$ x_1, x_2, ....$$但是替代仅仅为0或1，他的输入可以是任何0到1之间的值，所以，实例中, 0.638 ...是sigmoid神经元的一个有效输入，同样像感知器，sigmoid神经元对每个输入有自己的权重$$ w_1, w_2, ... $$, 和一个总偏置值bias, $$ b $$。但是输出不再是0或1，而是$$ \sigma(w\cdot x + b) $$，$$ \sigma $$被称之为$$ sigmoid function_* $$，他的定义是：

$$
\sigma(z) \equiv \frac{1}{1 + e^{-z}}
$$

显式的把全部放进去，sigmoid神经元的输出就变成了：

$$
\frac{1}{1 + exp(-\sum_jw_jx_j - b)}
$$

第一眼看去，sigmoid神经元跟感知器非常不一样，Sigmoid函数的代数形式看上不那么透明，如果你不熟悉可以禁用他，实际上sigmoid神经元和感知器有很多相似的地方，Sigmoid函数的代数形式产生更多的是技术细节，而不是真正的理解障碍。

为了了解感知器模型的相似性，假设$$ z \equiv w\cdot x + b $$是一个非常大的正数，那么$$ e^{-z} \approx 0 $$所以$$ \sigma(z) \approx 1 $$，换句话说当$$ z \equiv w\cdot x + b $$是很大的正数，sigmoid神经元的输出约等于1，他就好像感知器一样，假设另一方面，如果$$ z \equiv w\cdot x + b $$是一个非常大的负数，那么$$ e^{-z} \rightarrow \infty $$, $$ \sigma(z) \approx 0 $$, 所以当$$ z \equiv w\cdot x + b $$是个很大负数的时候，Sigmoid神经元也近乎约等于感知器，仅仅当$$ w\cdot x + b $$有适当的模型尺寸时，才会与感知器模型有比较大的偏差。

$$ \sigma $$的代数形式怎么样呢？我们怎么理解他？实际上他真实的形式并不是很重要 -- 真正重要的其实是绘出的他的函数形状，如下：

![Sigmoid]({{ site.url }}/static/deep-learning-chapt-1/Sigmoid.jpg)

这种形状是一个平缓输出版本的阶梯函数：

![Perceptron]({{ site.url }}/static/deep-learning-chapt-1/Perceptron.jpg)

如果$$ \sigma $$是个阶梯函数，那么sigmoid神经元将会是一个感知器，因为输出会根据$$ w\cdot x + b $$是正数或负数来决定是1或是0。

> 实际上, 当$$ w\cdot x + b = 0 $$，感知器输出0，阶梯函数输出为1，所以严格的说，我们需要修改阶梯函数的一头，但你需要一个想法。

通过使用我们得到的实际的$$ \sigma $$函数，我们发现，实际上$$ \sigma $$函数的平滑度才是他的重要因素，而不是他的具体形式，$$ \sigma $$函数的平滑度意味着在神经元中权重的小变动$$ \Delta w $$和偏置值的小变动$$ \Delta b $$会产生一个输出的小变动$$ \Delta \mbox{output} $$，计算告诉我们$$ \Delta \mbox{output} $$的值约是：

$$
\Delta \mbox{output} \approx \sum_j \frac{\partial \, \mbox{output}}{\partial w_j} \Delta w_j + \frac{\partial \, \mbox{output}}{\partial b}\Delta b
$$

这里总和大于所有的权重$$ w_j $$，$$ \partial \mbox{output}/\partial w_j $$和$$ \partial \mbox{output}/\partial b $$分别表示输出相对于$$ w_j $$和$$ b $$的偏导数。你如果对偏导数感到不舒服也不用惊慌！上面的表达式看上去很复杂，所有的偏导数实际上说的还是很简单的（这是个好消息）: $$ \Delta \mbox{output} $$在权重和偏置值中是针对$$ \Delta w_j $$和$$ \Delta b $$变化的一个线性函数($$ linear function $$), 这种线性特性让我们很容易的可以选择权重或偏置值的小改动来达到我们希望的输出值的小改动。所以当Sigmoid神经元和感知器拥有很多相同性质的行为时，它可以更轻松的指出输出变化是如何受到权重和偏置值变化影响的。

如果$$ \sigma $$他的形状真的很重要，而不是他的形式，那么为什么要用等式$$ \sigma(z) \equiv \frac{1}{1 + e^{-z}} $$, 实际上，在之后我们会偶尔的考虑下让神经元使用其他的激活函数$$ f(\cdot) $$来输出$$ f(w\cdot x + b) $$。当我们使用不同激活函数时，最主要变化的是等式$$ \Delta \mbox{output} \approx \sum_j \frac{\partial \, \mbox{output}}{\partial w_j} \Delta w_j + \frac{\partial \, \mbox{output}}{\partial b}\Delta b $$中特定值的偏导数。事实证明，但我们稍后计算偏导数的时候，使用$$ \sigma $$会简化代数，只是因为指数在微分时有它可爱的特性，无论如何，$$ \sigma $$通畅用于神经网络工作，他同样也是我们本书中常用的激活函数。

我们如何来描述一个sigmoid神经元的输出呢？很明显，sigmoid神经元与感知器的一个最大的不同就是输出不止为0或1，它可以输出任意0到1之间的真实数字，所以这些值可以是0.173 ... 和0.689 ...。这是很有用的，比如，我们希望从神经元网络的输出中展示出输入图片像素值的平均强度。但有时候也是令人讨厌的，假设我们希望从神经网络的输出来确定“输入图片是9”或者“输入图片不是9”，很明显，如果像感知器一样输出是0或1的时候是很容易做到的，但在实际当中，我们可以建立一个约定来处理他，比如，有任意至少大于0.5的输出来决定这是个“9”，有任意小于0.5的输出就“不是9”，当我们使用这个约定的时候，我总是会声明他，这样就不会引起混淆。

### Exercises 1

---

- **Sigmoid神经元模拟感知器，第一部分**：

假设我们让感知器网络中的所有权重和偏置值乘以一个正数常数$$ c > 0$$，证明网络的行为不会发生变化。

- **Sigmoid神经元模拟感知器，第二部分**：

假设我们有与最后一个问题相同的设置 - 感知器网络。假设同样的感知器网络全部的输入都已经被选择了，我们不一定要的是真实的输入址，我们只需要一个被修正过的值。假设在网络中任意特殊感知器的输入为$$ X$$，权重和偏置值为$$ w\cdot x + b \neq 0 $$, 现在将网络中所有的感知器替换成sigmoid神经元，然后乘上一个正的常数$$ c > 0 $$, 当$$ c \rightarrow \infty $$时证明sigmoid神经元网络跟感知器网络的行为几乎是一样的，当其中一个感知器$$ w\cdot x + b = 0 $$时他为什么会失效？

## The architecture of neural networks

---

在下一步分钟，我会介绍一个能很好分类手写数字的神经网络，为了做准备，我们来命名网络的不同部分，这对我们解释一些术语很有帮助，假设我们有如下网络：

![preparation-named-network]({{ site.url }}/static/deep-learning-chapt-1/preparation-named-network.png)

在之前提到过，网络最左边的这层叫输入层，这层中的神经元被称之为输入神经元，最右侧层或者说输出层，包含了输出神经元，在这个例子中就是一个单独的输出神经元，中间层我们称之为隐藏层，因为这层中的神经元既不是输入也不是输出，术语“hidden(隐藏)”听起来可能还有些神秘 -- 我第一次听说他的时候我觉得他肯定有一些很深的哲学或是数学的意义 -- 但事实上他她什么意义都没有，甚至都不能说“既不是输入也不是输出”，以上的网络里仅仅只有一层隐藏层，但是很多的其他网络也许有很多隐藏层，比如以下这个4层网络就有2个隐藏层：

![4-layer-network]({{ site.url }}/static/deep-learning-chapt-1/4-layer-network.png)

有些混乱，由于历史原因，这种多层网络有时候也被称之为多层感知器(Multilayer perceptrons)或者MLPs, 尽管它使用的是sigmoid神经元而不是感知器。我不会在这本书中使用MLP，因为我觉得他有点混乱，但是必须要提醒你他的存在。

通常在网络中定义输入和输出是简单的。比如我们要确定一个手写数字图片是否描述为9，一个很自然的方式来设计网络就是将图片编码成强壮的像素放到输入神经元中, 如果图片是一个64乘64的灰度图片，那么我们就需要$$ 4096 = 64 \times 64 $$的输入神经元，他们各自的强度在0到1之间适当的调整，输出层只需要包含一个神经元，输出值小于0.5表明他不是9，大于0.5表明它是9。

虽然神经网络的输入和输出层通常是简单的，但隐藏层的设计可以是非常艺术的。特别要说的是，我们不可能用一些简单的经验法则来总结隐藏层的设计过程。相反，神经网络研究人员已经为隐藏层开发了很多设计启发式，比如，这种启发式可以被用来指出如何针对需要的时间减少隐藏层来训练网络，我们会看到一些类似的设计启发式在稍后的文章中。

到目前为止，我们已经讨论了神经网络是一层的输出用来作为下一层的输入的，像这样的网络我们称之为前馈神经网络(feedforward)，这就意味着这个神经网络中是没有循环的 -- 信息总是向前的，从不会向后，如果我们将其循环，那么我们就必须结束这种将输出输入到$$ \sigma $$函数的情况，那就会变的很难讲通，所以这里我们不允许这种循环。

但是，也存在另外一种人工神经网络模型，他让反馈循环变得可能，我们称之为“循环神经网络”（[recurrent neural networks](https://en.wikipedia.org/wiki/Recurrent_neural_network))，这个模型的想法是在静止前的有限的持续的一段时间内激发神经元。那种激发可以刺激其他的神经元，而那些神经元可能也会在稍后激发，也可能持续有限。这也会导致更多的神经元激发，随着时间的推移，我们会得到一个级联的激发神经元群。在这个模型中，循环不引起问题，因为一个神经元的输出仅仅只在稍后的时间内影响他的输入，而不是瞬时的。

循环神经网络的影响力要比前馈神经网络的影响力小，一部分原因是因为他的学习算法还比较弱。（到目前为止）但是循环神经网络仍然是相当有趣的，他在思想上比前馈神经网络更接近于我们大脑的工作。而且他可能有时能解决一些前馈神经网络难以解决的重要问题。但是，为了缩小我们的范围，这本书中我们会更专注于广泛使用的前馈神经网络。

## A Simple network to classify handwritten digits

---

定义好了神经网络，现在让我们回归手写分类问题，我们可以拆分这个问题成2个小问题。首先，我们需要把一个包含了很多数字的图片拆成一系列只包含一个数字的单独的图片，比如我们想要把下面的图片拆分：

![digits-for-break]({{ site.url }}/static/deep-learning-chapt-1/digits-for-break.png)

将其变成6个单独的图片：

![digits-separate]({{ site.url }}/static/deep-learning-chapt-1/digits-separate.png)

我们人类可以很轻易的分解问题，但是这对电脑程序来说，正确的分割图片是一个挑战。当图片被真确分割后，程序就需要来分类没个单独的数字，所以，这个实例中，我们希望程序来识别上面的第一个数字，

![first-number-5]({{ site.url }}/static/deep-learning-chapt-1/first-number-5.png)

是一个“5”。

我们着重来写一个程序来解决第二个问题，分类这些单独的数字。事实证明当你有一个很好的分类数字的方法时，分割问题不是很难去解决，有很多方法可以去解决分割问题，其中之一就是尝试不同的分割法，利用单独的数字分类器来记录每次尝试行的分割。如果分类器对所有分割片段有把握，那么它会有一个高的评分，如果分类器在分类时在一个或多个片段上遇到麻烦，那么就会给个低分，这个想法是，如果分类器在哪遇到了麻烦，说明他的片段可能选错了，这个方法和其他更多样的方法都可以很好的用来解决切割问题。所以与其担心图片分割，我们会专注于开发一个可以解决更多有趣和复杂问题的神经网络，也就是识别单独的手写数字。

识别一个单独的数字，我们会用到一个3层神经网络：

![3-layer-network]({{ site.url }}/static/deep-learning-chapt-1/3-layer-network.png)

神经网络的输入层包含了图片编码后的所有值，如下一节所述，我们的网络的的训练数据是由臊面的手写数字，$$ 28 \times 28 $$像素的图片组成，所以输入层有$$ 784 = 28 \times 28 $$个神经元，简单起见，我已经省略了上图中784个输入中的大部分神经元，输入的像素是灰度的，0.0表示白色，1.0表示黑色，中间的值表示逐渐变黑的灰色。

第二层为隐藏层，我们能将隐藏层中的神经元个数用n表示，我们会尝试不同数值的n，这个例子当中展示了一个包含n=15个神经元的隐藏层。

网络的输出层包含了10个神经元，如果第一个神经元激发了，比如他的输出约等于1，那么那就说表明网络认为这个数字是0，如果第二个神经元激发了那么网络认为这个数字是1，以此类推。再说准确点，我们将输出神经元从0到9编号，并且指出哪个神经元有最大的激活值，如果哪个神经元是编号为6的神经元，那么网络会猜测输入的数字是6，其他输出神经元也是一样。

你可能会疑惑，为什么我们用了10个输出神经元，毕竟这个网络的目标是告诉我们输入的是哪个数字（0,1,2,...,9）。一个看似自然的方法是利用4个神经元，根据神经元输出是否接近0或1，将其视为一个二进制值。4个神经元足够用来编码结果了，因为$$ 2^4 = 16 $$比输入数字的10个可能的值还要多，为什么我们要用10个神经元来代替？是不是没有效率？最终的理由是经验性的：我们可以尝试两种网络的设计，他的结果会显示，10个输出神经元学习识别数字会比4个输出神经元好。但是这会留给我们疑问，为什么使用10个输出神经元会工作的更好，有没有一些启发式教育告诉我们应该用10个输出来编码还是用4个输出来编码？

来理解为什么我们这么做，他帮助我们理解神经网络从第一原则处做了什么，首先考虑下我们用的10歌输出神经元的情况，让我们集中精力在第一个输出神经元上，这个神经元决定了数字是否为0，他依靠权衡从隐藏层获得的证据来做到这些，那么这些隐藏层神经元是做什么的？那么，让我们设想下隐藏层的第一个神经元参数的目标是检查出图片是否如下图所呈现出的那样：

![first-hidden-layer-neuro]({{ site.url }}/static/deep-learning-chapt-1/first-hidden-layer-neuro.png)

它可以加重重叠部分图像输入像素的权重，减轻其他输入部分权重，同样的，让我们假设下第二、三和四个隐藏层神经元，他们的参数的目标也是检查图片的呈现是否如下：

![2-3-4-hidden-neuro]({{ site.url }}/static/deep-learning-chapt-1/2-3-4-hidden-neuro.png)

可能你已经猜到了，这4张图组成了数字0，就是之前那张一行数字中的一个数字：

![digit-0]({{ site.url }}/static/deep-learning-chapt-1/digit-0.png)

所以，如果全部的这4个隐藏神经元激发了，那么我们可以得出结论这个数字是0，当然，这一系列证据并不是我们能够得出图片是0的唯一证据，我们也可以从其他合理的途径来得到这个0（比如，通过上图的翻译，或者轻微的变形）。但似乎我们可以肯定地说，在这个例子中我们至少能得出结论输入的是0.

假设神经网络是以这种方式工作，我们就可以给出一个可信的解释，为什么10个输出好，而不是4。如果我们是有4个输出，那么第一个输出神经元将会尝试决定数字的最高有效位，然而目前并没有什么简单的方法将最高有效位与上述图片形状联系起来，很难想象，有一些好的历史因素是能够让数字组成形状与最高有效位密切相关联的。

现在，所有的这一切都仅仅只是个启发，没说三层神经网络必须要以我描述的这种利用隐藏层简单检测组建形状的方式来操作，也许一个更聪明的学习算法可以找到让我们只用4个输出神经元的权重赋值。但是作为一个启发式教育，我所描述的思维通常比较好，可以帮你节省很多时间来设计一个不错的神经网络架构。

### Exercises 2



---

未完待续...