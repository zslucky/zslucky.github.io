---
layout: math-post
title: Deep learning - Chapt 1 - Using neural nets to recognize handwritten digits
chineseTitle: 深度学习 - 第一章 - 使用神经网来识别手写数字
icon: exchange
date:   2017-03-26 10:50:57 +0800
categories:
  - python
tags:
  - deep-learning
---

## Overview

[转][译] [《Neural Networks and Deep Learning - Chapt 1》](http://neuralnetworksanddeeplearning.com/chap1.html) --- [Michael Nielsen](http://michaelnielsen.org/)

人类的视觉系统是这个世界上最为奇妙的事物之一，面对一些手写的数字，人们可以很轻易的识别它，然而这种轻松是具有欺诈性的。在我们每一半大脑中，人们都有一个主要的视觉皮层也就是我们已知的V1(初级视皮层，亦称纹状皮层)，包含了1.4亿个神经元以及100亿个相互作用的连接。当然，人类的视力还不仅仅只是是拥有V1，而是一整个系列的视觉皮层 -- V2、V3、V4和V5会连续渐进的进行复杂的图像处理。我们给我们的头脑携带了一个超级电脑，他经历了数亿年的进化，使自己适应并理解这个可视的世界，所以辨识手写数字并不简单。我们人类是神奇的，擅长于理解我们眼睛所看的东西，但是完成这些工作却是不经意间的，令人惊讶。所以我们通常无法领会我们的视觉系统如何解决这样困难的问题。

如果你尝试去写出一个类似以上描述的电脑程序来辨识数字，视觉辨识部分很明显是很困难的。在我们看来似乎很简单的东西突然变得极为困难，简单的凭直觉如何来辨识外形 - “一个数字9，上面有一个圈，右下方又一个竖线”，结果是并不能那么简单的在算法上表示出来。当你尝试去明确这些规则，你很快会迷失在一堆异议、警告和特殊情况的困境中，这是不希望的。

神经网络利用了不同的方法来着手处理这个问题，就是利用大量的手写数字作为已知的训练样本

![mnist_100_digits]({{ site.url }}/static/deep-learning-chapt-1/mnist_100_digits.png)

然后开发一个系统来学习这些训练样本，换句话说，神经网络利用样本来自动的推出规则来辨识手写数字，更进一步的说就是，利用不断增多的悬链样本，神经网络能够学到更多关于手写内容来提升准确率，因此我已经展示了以上100个训练数字，也许我们可以利用成千上万甚至上亿的训练样本来创建一个更好的手写识别器。

在这一章节中，我们会编写一个电脑程序来实现这个神经网络来学习识别手写数字，这段程序只有74行，不利用任何神经网络库，但这个小程序能够在没有人介入并拥有96%准确率的情况下来识别手写数字，进一步说在之后的章节中我们会开发新的主意来提升性能至99%以上。实际上，最好的商业神经网络现在已经被很好的使用在银行中来处理支票，利用邮件办公室来识别地址。

我们致力于手写内容识别是因为它可以作为一个通用的神经网络模型，他可以用来开发更高级的技术，比如深度学习。在之后的内容中，我们会讨论这些点子是如何被应用在计算机视觉、语音、自然语言处理甚至是其他领域。

当然，如果只是为了写一个程序来识别手写数字，那么这章节就太短了！所以沿途我们会开发许多关关于神经网络键的点子，包括2个重要的人造神经元类型(感知器和sigmoid神经元)，还有标准的神经网络学习算法，如已知的SGD(随机梯度下降算法)，全文我都会专注于解释为什么事物会被这样完成，还有建立你的神经网络直觉。比起如果我只是陈述基础的原理，那更需要一个漫长的讨论，但是对你达到深入理解是很有价值的，在章节结束我们能够理解什么是深度学习，还有为什么他有影响。

---

## Contents

- [**Perceptrons** - 感知器](#perceptrons)
- [**Sigmoid Neurons** - Sigmoid神经元](#sigmoid-neurons)

## Perceptrons - 感知器

---

什么是神经网络？作为开始，我会解释一个叫做感知器的人工神经，感知器是在1950和1960年由[Frank Rosenblatt](https://en.wikipedia.org/wiki/Frank_Rosenblatt)开发，受到更早[Warren McCulloch](https://en.wikipedia.org/wiki/Warren_Sturgis_McCulloch)和][Walter Pitts](https://en.wikipedia.org/wiki/Walter_Pitts)的启发。如今它已经非常普遍的在其他模型中使用人工神经元 - 在本书中，并且在大多数神经网络模型中，主要使用的是一个叫Sigmoid神经元的东西。我们会对sigmoid有个简短的接触，但是可以让我们了解为什么sigmoid神经元被定义为这种工作方式，让我们一起来花点时间来看下第一个感知器。

感知器怎么工作？一个感知器有多个二进制输入$$ x_1 $$, $$ x_2 $$, ..., 并且处理成单独一个二进制输出：

![First-perceptron]({{ site.url }}/static/deep-learning-chapt-1/first-perceptron.png)

这个例子中显示的感知器有3个输入$$ x_1 $$, $$ x_2 $$和$$ x_3 $$，通常来说它可以有更多的输入，Rosenblatt提出一个简单的规则来计算输出，他介绍了权重$$ \text{weights } $$, $$ w_2 $$和$$ w_2 $$, ...，真实的数字表示各个输入对于输出的重要性，神经元的输出是0或1由权重总和$$ \sum_jw_jx_j $$是否小于或大于某个阈值来决定，就比如权重，这个阈值是神经元的一个真实的数据，明确一点的代数表达式如下：

$$
output =
\begin{cases}
0 & \text{if } \sum_jw_jx_j  \leq \text{ threshold} \\
1 & \text{if } \sum_jw_jx_j  > \text{ threshold}
\end{cases}
$$

这感知器如何工作的全部。

这是个基础的数学模型，你可以把感知器认为是一个可以权衡证据来做决策的设备，这还不是一个非常现实的示例，但他可以让你简单的理解，我们很亏会接触一个更现实的例子。假设下周末即将来临，你已经听说了在你的城市会举办一个国际象棋活动，你也喜欢国际象棋，你在尝试做决定是否要参加这个活动，你做决定可能会权衡以下3个因素：

1. 天气是否晴朗？
2. 你的男／女朋友是否陪你一起？
3. 活动地点是否在公共交通设施附近？（你没有自己的车）

我们可以把这3个因素展示成二进制的变量$$ x_1 $$, $$ x_2 $$和$$ x_3 $$, 在这个实例中，如果天气好我们有$$x_1 = 1 $$, 如果天气不好我们有$$x_1 = 0$$, 同样的$$ x_2 = 1$$代表如果你的男／女朋友也想去，而$$ x_2 = 0$$表示不去，同样的也可以用在公共交通设施上。

现在，假设你是绝对的喜欢国际象棋，甚至说你去参加活动无关男／女朋友他是否对活动感兴趣或者去达场地有多困难，但是你真的非常讨厌坏天气，而且如果天气不好你没有办法去活动场地，你可以使用感知器来完成这个决策模型，一种发式是选择天气的权重$$ w_1 = 6 $$, 其他的设为$$ w_2 = 2, w_3 = 2 $$，$$ w_1 $$这个更大点的说字说明了他影响你更多，比男女朋友陪你参加货距离公共交通设施更近更重要，最后，你选择了一个阈值为5的感知器，根据这些选择，感知器得到了一个决策模型，只要天气好就输出1，只要天气差就输出0，这个无关另外两个条件。

利用多样的权重和阈值，我们可以得到不同的决策模型，比如我们利用阈值3来代替，那么我们的感知器会决定当天气是好的或者另外两个条件同时满足时你去参加活动，换句话说，这就是一个不同的决策模型了，降低阈值代表了你有更强烈的欲望去参加活动。

很明显，感知器不适宜颚完成的人类决策模型，但这个例子阐明了感知器是如何权衡各种类型的证据来做决策的，并且一个复杂的感知器网络可以做一个相当精确的决策，这个决策还是比较可信的。

![Perceptron-Network]({{ site.url }}/static/deep-learning-chapt-1/multi-perceptron-network.png)

在这个网络中，第一列感知器（我们称之为第一层感知器），会根据输入的证据做3个非常简单的决策，那么第二层感知器呢？每一个感知器会权衡第一层做的决定结果做一个决策，这样的话第二层的感知器可以做一个比第一层感知器更复杂更抽象级别的决策，而且还可以在第三层感知器上做更复杂的决定，这么一来，一个多层感知器网络可以作用于更复杂的决策。

顺带一提，当我定义多个感知器的时候我说过一个感知器直邮一个输出，在上面的网络中感知器看上去有多个输出，实际上她们仍然是单个输出，这多个输出箭头仅仅只是表示某个感知器的输出作为其他多个感知器的输入，

让我们简化感知器的描述，条件$$ \sum_jw_jx_j > \text{threshold} $$是笨重的，我们可以创造2个计数变化来简化它，第一个变化是将$$ \sum_jw_jx_j $$写成一个点积$$ w\cdot x = \sum_jw_jx_j $$, 这里$$ w $$和$$ x $$是权重和输入的向量，第二个改变是将$$ \text{threshold} $$改动到不等式的另一边，并且把它改成我们已知的感知器的偏置值$$ bias $$，$$ b \equiv -\text{threshold} $$，利用偏置值来代替阈值，感知器规则可以被重写成：

$$
output =
\begin{cases}
0 & \text{if } w\cdot x + b  \leq 0 \\
1 & \text{if } w\cdot x + b  > 0
\end{cases}
$$

你可以将偏置值想成是一个感知器有多简单输出1的测量标准，或者说测量一个感知器有多容易被触发，当感知器拥有一个相当大的偏置值时，他会非常容易的输出1，但是当它为负时，感知器就很难输出1了，很明显偏置值在我们描述感知器的时候只会有一个小的改变，但是我们会在之后看到它进一步简单化计算。正是因为如此，这本书中我们不会再用阈值，我们会一直使用偏置值$$ bias $$。

我已经描述过感知器是一个权衡证据来做决策的方法，另一方面，感知器可以被用来计算我们经常隐藏于计算下的基础逻辑功能，功能比如 AND, OR和NAND，举个例子，假设我们友谊颚感知器有两个输出，权重都是-2，并且全部的偏置值时3，如下图：

![elementary-logical-function]({{ site.url }}/static/deep-learning-chapt-1/elementary-logical-function.png)

然后我们输入00，便输出了1，因为$$ (-2) * 0 + (-2) * 0 + 3 = 3 $$是正数，我已经介绍过符号*是显式的乘法，同样的计算输入为01和10时产出1，但是输入为11的时候产出是0，因为$$ (-2) * 1 + (-2) * 1 + 3 = -1 $$是一个负值，因此我们的感知器实现了一个NAND（与非门）！

NAND与非门例子表明我们可以用感知器来计算简单的逻辑功能，实际上我们可以使用的感知器网络来计算任何逻辑功能，原因是因为与非门是通用的计算，那就是说，我们可以基于与非门创立任何的计算，比如，我们可以利用与非门来创建一个添加2个比特的电路，$$ x_1 $$和$$ x_2 $$，这就需要进行按位求和(bitwise sum), $$ x_1 \bigoplus x_2 $$，同样的当$$ x_1 $$和$$ x_2 $$同为1将会被设置为1时进位位(carry bit)会被设置成1，就好比进位位时它们的积$$ x_1x_2 $$：

![bitwise-and-carry]({{ site.url }}/static/deep-learning-chapt-1/bitwise-and-carry.png)

为了得到一个相等的感知器网络，我们将所有的与非门替换成2个输入的感知器，每个的权重为-2偏置值为3，以下就是这个网络：

![replaced-nand-network]({{ site.url }}/static/deep-learning-chapt-1/replaced-nand-network.png)

感知器网络中一个值得注意的方面是最左边的感知器的输出被2次用来作为最下边感知器的输入，当我定义感知器模型时，我不曾说过是否这种“两次输出到同一地方”是被允许的，实际上，这没什么太大关系，如果我们不希望允许这类事，那么我们也许可以简单的将两条线合并到一条中去，并且设置权重为-4来代替-2（如果你没很明显的发现，你因该先停下来然后证明给你自己看这是相等的），根据这些改动，这图看上去如下，所有未标记的权重都是-2，所有偏置值都是3，仅有一个权重为-4:

![merged-output-network]({{ site.url }}/static/deep-learning-chapt-1/merged-output-network.png)

到目前为止，我已经在感知器左边绘制出了像$$ x_1 $$和$$ x_2 $$这样的变量，实际上给感知器在额外绘制一个层更符合传统 -- 输入层，来给输入编码：

![input-layer-network]({{ site.url }}/static/deep-learning-chapt-1/input-layer-network.png)

这个感知器输入的符号它有一个输出，但是没有输入：

![input-layer]({{ site.url }}/static/deep-learning-chapt-1/input-layer.png)

这是一个简化部分，他并不意味着感知器可以没有输入，看这个，我们只是假设有这么一个感知器没有输入，然后这个权重和总是为0，$$ \sum_jw_jx_j $$, 因此如果$$ b > 0 $$感知器会输出1，如果$$ b \leq 0 $$感知器会输出0，就是说感知器会简单的输出一个修正过的数值，而不是所期望的值(比如之前的$$ x_1 $$), 因此我们可以更好的把输入感知器不当成感知器，但是是一个更特殊的单元，他只是简单的定义了一个我们所期望的输出值$$ x_1, x_2, $$...

这个计数器例子展示了感知器网络如何使用多个NAND与非门来模拟电路，因为与非门是普遍的计算，所以感知器也是通用的计算。

这种感知器的通用性计算，既让我们安心又让我们失望，让我们安心是因为他告诉我们感知器网络可以像其他计算设备一样强大，但是同样也让我们失望是因为他让我们看起来感知器仅仅只是一个新的与非门，他几乎不是什么大的新东西！

但情况比这篇陈述的观点要好，他给了我们一个结果，我们可以设计一个自动调整人工神经元网络权重weights和偏置值biases的学习算法，这个调整发生在受到外部刺激时，而不是有程序人员的直接介入，这种学习算法让我们有了一种方式能够使用与传统逻辑门完全不同的人工神经元，替代显式的与非门或是其他门展示的电路，我们的神经网络能很轻松的来学习解决问题，有时候直接的设计传统电路时非常困难的一个问题。

## Sigmoid Neurons - Sigmoid神经元

---

